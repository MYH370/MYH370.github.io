<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>Hexo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  
    <link rel="shortcut icon" href="/favicon.png">
  
  
<link rel="stylesheet" href="/css/style.css">

<meta name="generator" content="Hexo 6.2.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Hexo</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
          <a class="main-nav-link" href="/categories">Categories</a>
        
          <a class="main-nav-link" href="/tags">Tags</a>
        
          <a class="main-nav-link" href="/about">About</a>
        
      </nav>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main">
  
    <article id="post-Kafka环境搭建-1" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2022/06/11/Kafka%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA-1/" class="article-date">
  <time class="dt-published" datetime="2022-06-11T04:43:53.000Z" itemprop="datePublished">2022-06-11</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2022/06/11/Kafka%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA-1/">Kafka环境搭建</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p><strong>长春建筑学院</strong></p>
<p><strong>《kafka应用实训》</strong></p>
<p><strong>选题及任务书</strong></p>
<p>人工智能产业学院<br><strong>一、课程设计选题</strong></p>
<p><strong>项目一：在个人博客中继续发布kafka相应学习内容（必选）</strong></p>
<p>1．项目来源及背景</p>
<p>随着协同办公的迅速发展，掌握分布式的版本控制方法成为了必然要求。如何使用当今最为流行的Github进行版本控制也成为了大数据专业人才的必要条件。本项目旨在让学生学会Git的同时，掌握Github的基本操作，并可以使用简单的建站工具在Github上进行个人博客的部署，完成相应的知识输出。</p>
<p>2．项目需求分析</p>
<p>（1）个人博客采用Hexo进行相应搭建。地址：<a target="_blank" rel="noopener" href="https://hexo.io/zh-cn/">https://hexo.io/zh-cn/</a></p>
<p>（2）在使用Hexo前，需要安装node和npm。</p>
<p>（3）使用Hexo，选择相应的主题进行博客搭建。</p>
<p>（4）具体搭建步骤在如下示例中。</p>
<p>示例：<a target="_blank" rel="noopener" href="https://skykip.github.io/2022/04/10/hello-world/">https://skykip.github.io/2022/04/10/hello-world/</a></p>
<p>所采用主题为：Wikitten</p>
<p><strong>（5）请各位同学在已经搭建完成的个人博客中，将之前kafka课程中做的分组任务（Kafka全流程配置与应用），以博文的形式发布到自己的博客中进行分享。</strong></p>
<p>其中：</p>
<p>a) Kafka环境配置作为一篇博文，</p>
<p>名称为：《Kafka环境配置》</p>
<p>b) Kafka命令行操作作为一篇博文，</p>
<p>名称为：《Kafka命令行操作》</p>
<p>c) 生产者API，消费者API，Topic管理API作为一篇博文，</p>
<p>名称为：《kafka API使用方法》</p>
<p><strong>项目二：在kafka集群中部署Eagle运维监控（必选）</strong></p>
<p>1．项目来源及背景</p>
<p>kafka 自身并没有继承监控管理系统, 因此对 kafka 的监控管理比较不便, 好在有大量的第三方监控管理系统来使用,常见的有: </p>
<p>l Kafka Eagle </p>
<p>l KafkaOffsetMonitor </p>
<p>l Kafka Manager(雅虎开源的 Kafka 集群管理器) </p>
<p>l Kafka Web Console </p>
<p>l 还有 JMX 接口自开发监控管理系统</p>
<p>其中，Kafka Eagle最为流行，本项目则希望同学们可以成功部署Kafka Eagle，完成对kafka集群的基本监控。</p>
<p><img src="file:///C:\Users\雷神\AppData\Local\Temp\ksohtml9240\wps29.jpg" alt="img"> </p>
<p>2．系统需求分析</p>
<p>官网：<a target="_blank" rel="noopener" href="https://www.kafka-eagle.org/">https://www.kafka-eagle.org/</a></p>
<p>（1） 安装包下载地址: </p>
<p><a target="_blank" rel="noopener" href="https://github.com/smartloli/kafka-eagle-bin/">https://github.com/smartloli/kafka-eagle-bin/</a>		</p>
<p><strong>（安装包也已上传至学习通的资料中）</strong></p>
<p>（2） 上传安装包，解压</p>
<p>（3） 配置环境变量:JAVA_HOME 和 KE_HOME</p>
<p>vi &#x2F;etc&#x2F;profile </p>
<p>export JAVA_HOME&#x3D;&#x2F;usr&#x2F;java&#x2F;jdk1.8</p>
<p>export PATH&#x3D;$PATH:$JAVA_HOME&#x2F;bin</p>
<p>export KE_HOME&#x3D;&#x2F;export&#x2F;server&#x2F;kafka-eagle</p>
<p>export PATH&#x3D;$PATH:$KE_HOME&#x2F;bin</p>
<p>（4） 配置 KafkaEagle</p>
<p>cd &#x2F;export&#x2F;server&#x2F;kafka-eagle&#x2F;conf </p>
<p>vi system-config.properties</p>
<p>l 需要更改的地方：</p>
<p>kafka.eagle.zk.cluster.alias&#x3D;cluster1</p>
<p>cluster1.zk.list&#x3D;node1:2181,node2:2181,node3:2181</p>
<p>cluster1.kafka.eagle.broker.size&#x3D;3</p>
<p>kafka.eagle.url&#x3D;jdbc:sqlite:&#x2F;export&#x2F;data&#x2F;db&#x2F;ke.db</p>
<p>l 启动前需要手动创建&#x2F;export&#x2F;data&#x2F;db目录</p>
<p>mkdir &#x2F;export&#x2F;data&#x2F;db</p>
<p>（5） 启动Eagle</p>
<p>&#x2F;export&#x2F;server&#x2F;kafka-eagle&#x2F;bin&#x2F;ke.sh start</p>
<p>（6） <strong>对Eagle中的各项功能进行学习，并上传至博客中。同时也需在实训设计文档中对相应内容进行说明</strong>。</p>
<p><strong>项目三：使用<strong><strong>S</strong></strong>pring<strong><strong>B</strong></strong>oot搭建基础的Kafka流处理平台（必选）</strong></p>
<p>1．项目来源及背景</p>
<p>Kafka是目前主流的流处理平台，同时作为消息队列家族的一员，其高吞吐性作为很多场景下的主流选择。同时作为流处理平台，在大数据开发中，作为黏合剂串联各个系统。在本项目中，可以发布或订阅数据流处理系统，类似于消息队列数据流存储的平台，并且具备错误容忍当数据产生时就对数据进行处理。</p>
<p>2．需求分析</p>
<p>（1） 通过Github同步kafka_rgzn_Training仓库到本地</p>
<p><a target="_blank" rel="noopener" href="https://github.com/skykip/kafka_rgzn_Training/tree/main/kafka_rgzn_Training">https://github.com/skykip/kafka_rgzn_Training/tree/main/kafka_rgzn_Training</a></p>
<p>（2） 调试并启动项目，理解项目中共计11个代码文件的相应内容。</p>
<p><img src="file:///C:\Users\雷神\AppData\Local\Temp\ksohtml9240\wps30.jpg" alt="img"> </p>
<p>注：老师调试时使用的是jdk1.8.0_321</p>
<p><img src="file:///C:\Users\雷神\AppData\Local\Temp\ksohtml9240\wps31.jpg" alt="img"> </p>
<p>图1 项目结构</p>
<p><strong>调试时，在<strong><strong>KafkaDemoApplication</strong></strong>文件内启动项目。使用postman等Http接口调试工具对项目进行测试。</strong></p>
<p>A. <strong>测试时，使用get测试</strong><a target="_blank" rel="noopener" href="http://localhost:8080/kafka/send">**http://localhost:8080/kafka/**<strong>index</strong></a><strong>接口是否正常。发送相应的json数据。请截图Http接口调试工具中响应体的具体内容和项目运行界面的显示内容。如下图所示：</strong></p>
<p><img src="file:///C:\Users\雷神\AppData\Local\Temp\ksohtml9240\wps32.jpg" alt="img"> </p>
<p>图2 get响应体的具体内容</p>
<p>B. <strong>测试时，使用post方式向</strong><a target="_blank" rel="noopener" href="http://localhost:8080/kafka/send"><strong>http://localhost:8080/kafka/send</strong></a><strong>接口发送json数据。</strong></p>
<p><strong>样例数据如下（请在样例数据中改为自己的姓名和学号）：</strong></p>
<p>**{“title”:”<strong><strong>bigdata</strong></strong>19-<strong><strong>kafka”</strong></strong>,”body”:”2022-06-05<strong><strong>，于智龙（学号）的kafka实训”</strong></strong>}**</p>
<p><strong>请截图Http接口调试工具中响应体的具体内容和项目运行界面的显示内容。如下图所示：</strong></p>
<p><img src="file:///C:\Users\雷神\AppData\Local\Temp\ksohtml9240\wps33.jpg" alt="img"> </p>
<p>图2 post响应体的具体内容</p>
<p><img src="file:///C:\Users\雷神\AppData\Local\Temp\ksohtml9240\wps34.jpg" alt="img"> </p>
<p>图4 项目运行界面的显示内容</p>
<p>（3） 在实训文档中编写上述代码的调试文档。</p>
<p>要求：</p>
<p>1） 说明该项目完成的主要功能</p>
<p>2） 每个代码文件都要独立进行说明</p>
<p>3） 说明每个代码文件的具体用途</p>
<p>4） 对代码进行详细注释</p>
<p><strong>二、设计要求</strong></p>
<p>每个小组必须选择项目一，项目二与项目三，相同题目者设计报告不能雷同；</p>
<p>2.课程设计报告要做到层次清晰，论述清楚，图表正确，书写工整；</p>
<p>3.成绩根据平时、设计报告和答辩给定。</p>
<p><strong>三．设计任务</strong></p>
<p>1.使用Hexo + Github，选择主题，搭建个人博客。</p>
<p>2.将kafka相应的配置内容作为博文在个人博客上进行发布。</p>
<p>3.部署kafka-eagle运维监控系统，将部署方法和运维监控的相应内容写入博客，同时也在kafka实训设计文档中进行说明。</p>
<p>4.使用SpringBoot搭建基础的Kafka流处理平台。</p>
<p>5.对Kafka流处理平台进行相应测试，对各代码文件中各模块功能进行细致说明，对代码做详细注释。</p>
<p>7.请同学们在设计时注意存储备份，以免机器故障、或其他误操作而丢失程序。</p>
<p>8.在硬盘上建立自己的目录，将程序文件存储到该目录下，即有利于调试文件，又保证了文件的安全性。</p>
<p><strong>四．设计参考资料</strong></p>
<p>《Kafka权威指南》Neha Narkhede，Gwen Shapira，Todd Palino著，薛命灯 译、人民邮电出版社、2017年</p>
<p><a target="_blank" rel="noopener" href="https://yelog.org/2017/03/23/3-hexo-instruction/">https://yelog.org/2017/03/23/3-hexo-instruction/</a></p>
<p><a target="_blank" rel="noopener" href="https://yelog.org/2016/10/02/git-command/">https://yelog.org/2016/10/02/git-command/</a></p>
<p><a target="_blank" rel="noopener" href="https://wiki.zthxxx.me/wiki/index/">https://wiki.zthxxx.me/wiki/index/</a></p>
<p><a target="_blank" rel="noopener" href="https://ohlia.github.io/Wiki-site/wiki/Hexo/build-blog-by-hexo/">https://ohlia.github.io/Wiki-site/wiki/Hexo/build-blog-by-hexo/</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2022/06/11/Kafka%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA-1/" data-id="cl4ao192t0000akw2gk5ma421" data-title="Kafka环境搭建" class="article-share-link">Share</a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-Kafka环境搭建" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2022/06/11/Kafka%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/" class="article-date">
  <time class="dt-published" datetime="2022-06-11T04:42:12.962Z" itemprop="datePublished">2022-06-11</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>Kafka</p>
<p>分组问题报告</p>
<p>班级：      大数据1901   </p>
<p>学号：      194800132 194800117 194800118  </p>
<p>​            194800123                    </p>
<p>姓名：      陈利明 金文祺 周亚东 马永航             </p>
<p>指导教师：      于智龙       </p>
<table>
<thead>
<tr>
<th>分组问题</th>
<th>1</th>
<th>题目</th>
<th>Kafka全流程配置与应用</th>
</tr>
</thead>
<tbody><tr>
<td>问题目的</td>
<td>熟练操作Kafka相关配置、并对相应配置流程、应用逻辑进行梳理。</td>
<td></td>
<td></td>
</tr>
<tr>
<td>问题内容</td>
<td>请各组同学，按以下要求完成一份《Kafka全流程配置与应用》文档。 要求：文档使用word进行编辑（word模板如下）。配置文档需从Kafka环境配置开始，可借鉴课程中的文档进行编写。编写顺序为：1. Kafka环境配置 -&gt;2. Kafka命令行操作 -&gt;3. 生产者API -&gt;4. 消费者API -&gt;5. Topic管理API</td>
<td></td>
<td></td>
</tr>
<tr>
<td>问题进度</td>
<td>本次共有  5  个练习（具体到每个小练习），完成  5   个</td>
<td></td>
<td></td>
</tr>
<tr>
<td>问题设计</td>
<td>任务分配Kafka环境配置陈利明Kafka命令行操作陈利明生产者API周亚东、陈利明消费者API马永航、陈利明Topic管理API金文祺、陈利明 一、Kafka环境配置1.上传kafka_2.11-2.0.0.tgz到export&#x2F;server，使用tar -zxvf kafka_2.11-2.0.0.tgz命令解压<img src="file:///C:\Users\雷神\AppData\Local\Temp\ksohtml9240\wps1.jpg" alt="img"> <img src="file:///C:\Users\雷神\AppData\Local\Temp\ksohtml9240\wps2.jpg" alt="img"> 2. 对解压后的Kafka创建软连接方便访问ln -s &#x2F;export&#x2F;server&#x2F;kafka_2.11-2.0.0 &#x2F;export&#x2F;server&#x2F;kafka<img src="file:///C:\Users\雷神\AppData\Local\Temp\ksohtml9240\wps3.jpg" alt="img">  3. 修改 &#x2F;export&#x2F;server&#x2F;kafka&#x2F;config路径下的配置文件 编辑vi server.properties 修改以下内容broker.id&#x3D;0.从0开始,每台不能重复.Listeners &#x3D; plaintext:&#x2F;&#x2F;:9092通信用的是明文传递改成：Listeners &#x3D; plaintext:&#x2F;&#x2F;node1:9092设置数据存储的目录路径log.dirs&#x3D;&#x2F;export&#x2F;server&#x2F;data&#x2F;kafka-logs默认分区数num.partitions &#x3D; 1指定 zk 集群地址zookeeper.connect&#x3D;node1:2181,node2:2181,node3:2181<img src="file:///C:\Users\雷神\AppData\Local\Temp\ksohtml9240\wps4.jpg" alt="img"> 4. 配置环境变量 &#x2F;etc&#x2F;profile<img src="file:///C:\Users\雷神\AppData\Local\Temp\ksohtml9240\wps5.jpg" alt="img"> source &#x2F;etc&#x2F;profile重新加载环境变量5. cd &#x2F;export&#x2F;server分别把配置文件和环境变量的配置文件分发到node2;node3结点上具体命令如下（内容过长不便截图）scp -r &#x2F;export&#x2F;server&#x2F;kafka&#x2F; node2:$PWDscp -r &#x2F;export&#x2F;server&#x2F;kafka&#x2F; node3:$PWDscp -r &#x2F;etc&#x2F;profile&#x2F; node2:etc&#x2F;profilescp -r &#x2F;etc&#x2F;profile&#x2F; node2:etc&#x2F;profile6. 修改node2和node3的配置文件&#x2F;export&#x2F;server&#x2F;kafka&#x2F;config&#x2F;server.propertiesbroker.id&#x3D;1listeners &#x3D; plaintext:&#x2F;&#x2F;node2:9092<img src="file:///C:\Users\雷神\AppData\Local\Temp\ksohtml9240\wps6.jpg" alt="img"> broker.id&#x3D;2listeners &#x3D; plaintext:&#x2F;&#x2F;node3:9092<img src="file:///C:\Users\雷神\AppData\Local\Temp\ksohtml9240\wps7.jpg" alt="img">  7. 启动Kafka使用脚本一键启动<img src="file:///C:\Users\雷神\AppData\Local\Temp\ksohtml9240\wps8.jpg" alt="img">  二、Kafka命令行操作1. Kafka 中提供了许多命令行工具(位于$KAFKA HOME&#x2F;bin 目录下)用于管理集群的变更。<img src="file:///C:\Users\雷神\AppData\Local\Temp\ksohtml9240\wps9.jpg" alt="img"> 2. 查看当前可用topic.&#x2F;kafka-topics.sh –zookeeper node1:2181,node2:2181,node3:2181 –create –replication-factor 2 –partitions 2 –topic test<img src="file:///C:\Users\雷神\AppData\Local\Temp\ksohtml9240\wps10.jpg" alt="img">  3. 创建topicbin&#x2F;kafka-topics.sh –create –topic tpc_2 –partitions 2 –replication-factor 2 –zookeeper node1:2181该方式下,命令会自动判断所要创建的 topic 的分区数及副本数<img src="file:///C:\Users\雷神\AppData\Local\Temp\ksohtml9240\wps11.jpg" alt="img"> <img src="file:///C:\Users\雷神\AppData\Local\Temp\ksohtml9240\wps12.jpg" alt="img"> 4. 删除topicbin&#x2F;kafka-topics.sh  –delete –topic tpc_3 –zookeeper node1：2181删除 topic,需要一个参数处于启用状态: delete.topic.enable &#x3D; true,否则删不掉<img src="file:///C:\Users\雷神\AppData\Local\Temp\ksohtml9240\wps13.jpg" alt="img"> <img src="file:///C:\Users\雷神\AppData\Local\Temp\ksohtml9240\wps14.jpg" alt="img"> 5.  查看topickafka-topics.sh  –delete –topic tpc_1 –zookeeper node1:2181<img src="file:///C:\Users\雷神\AppData\Local\Temp\ksohtml9240\wps15.jpg" alt="img"> 6. 增加分区数bin&#x2F;kafka-topics.sh –alter –topic tpc_1 –partitions 3 –zookeeper node1:2181需要注意：Kafka 只支持增加分区,不支持减少分区原因是:减少分区,代价太大(数据的转移,日志段拼接合并) 如果真的需要实现此功能,则完全可以重新创建一个分区数较小的主题,然后将现有主题中的消息按照既定的逻辑复制过去; 7. 动态配置topic参数添加、修改配置参数bin&#x2F;kafka-configs.sh –zookeeper node1:2181 –entity-type topics –entity-name tpc_1 –alter –add-config compression.type&#x3D;gzip<img src="file:///C:\Users\雷神\AppData\Local\Temp\ksohtml9240\wps16.jpg" alt="img"> 删除配置参数bin&#x2F;kafka-configs.sh –zookeeper node1:2181 –entity-type topics –entity-name tpc_1 –alter –delete-config compression.type<img src="file:///C:\Users\雷神\AppData\Local\Temp\ksohtml9240\wps17.jpg" alt="img"> 8. Kafka命令行生产者与消费者操作(1)生产者:kafka-console-producerbin&#x2F;kafka-console-producer.sh –broker-list node1:9092, node2:9092, node3:9092 –topic tpc_1<img src="file:///C:\Users\雷神\AppData\Local\Temp\ksohtml9240\wps18.jpg" alt="img">  (2)消费者:kafka-console-consumer1消费消息bin&#x2F;kafka-console-consumer.sh –bootstrap-server node1:9092, node2:9092, node1:9092 –topic tpc_1 –from-beginning(从头开始)<img src="file:///C:\Users\雷神\AppData\Local\Temp\ksohtml9240\wps19.jpg" alt="img"> 2指定要消费的分区,和要消费的起始 offset bin&#x2F;kafka-console-consumer.sh–bootstrap-servernode1:9092,node2:9092,node3:9092 –topic tcp_1 –offset 2 –partition 0 9. Kafka命令行配置管理bin&#x2F;kafka-configs.sh zookeeper node1: 2181 –describe –entity-type topics –entity-name tpc_1 <img src="file:///C:\Users\雷神\AppData\Local\Temp\ksohtml9240\wps20.jpg" alt="img"> 比如查看 broker 的动态配置可以按如下方式执行:bin&#x2F;kafka-configs.sh zookeeper node1: 2181 –describe –entity-type brokers –entity-name 0 –zookeeper node1:2181<img src="file:///C:\Users\雷神\AppData\Local\Temp\ksohtml9240\wps21.jpg" alt="img"> 三、生产者APIProduce流程图<img src="file:///C:\Users\雷神\AppData\Local\Temp\ksohtml9240\wps22.png" alt="img"> 1.一个正常的生产逻辑需要具备以下几个步骤(1)配置生产者客户端参数及创建相应的生产者实例(2)构建待发送的消息(3)发送消息(4)关闭生产者实例创建一个maven工程并导入相关依赖<img src="file:///C:\Users\雷神\AppData\Local\Temp\ksohtml9240\wps23.jpg" alt="img"> 采用默认分区方式将消息散列的发送到各个分区当中（代码图片） Demo详细解释import org.apache.kafka.clients.producer.KafkaProducer; import org.apache.kafka.clients.producer.Producer; import org.apache.kafka.clients.producer.ProducerRecord; import java.util.Properties; public class MyProducer { public static void main(String[ ] args) throws InterruptedException { Properties props &#x3D; new Properties(); &#x2F;&#x2F;设置 kafka 集群的地址\props.put(“bootstrap.servers”, “node1:9092,node2:9092,node3:9092”);&#x2F;&#x2F;ack 模式,取值有 0,1,-1(all) , all 是最慢但最安全的，0不等响应就继续发（可靠性低），1leader会写到本地日志后，然后给响应，producer拿到响应才继续发（follwer还没同步）props.put(“acks”, “all”); props.put(“retries”, 3); &#x2F;&#x2F;失败重试次数-&gt;失败会自动重试（可恢复&#x2F;不可恢复）–&gt;(有可能会造成数据的乱序)props.put(“batch.size”, 10); &#x2F;&#x2F;数据发送的批次大小à提高效率&#x2F;吞吐量à太大会数据延迟props.put(“linger.ms”, 10000); &#x2F;&#x2F;消息在缓冲区保留的时间,超过设置的值就会被提交到服务端props.put(“max.request.size”,10); &#x2F;&#x2F;数据发送请求的最大缓存数props.put(“buffer.memory”, 10240); &#x2F;&#x2F;整个 Producer 用到总内存的大小,如果缓冲区满了会提交数据到服务端&#x2F;&#x2F;buffer.memory 要大于 batch.size,否则会报申请内存不足的错误à降低阻塞的可能性在创建真正的生产者实例前需要配置相应的参数,比如需要连接的 Kafka 集群地址。在 Kafka 生产者客户端 KatkaProducer 中有 3 个参数是必填的。bootstrap.servers key.serializer value.serializer为了防止参数名字符串书写错误,可以使用如下方式进行设置: props.setProperty(ProducerConfig.INTERCEPTOR_CLASSES_CONFIG,ProducerInterceptorPrefix.class.getName());props.setProperty(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG,”node1:9092,node2:9092”); props.setProperty(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG,StringSerializer.class.getName()); （运行结果图片2）2. 生产者api参数发送方式（发后即忘）发后即忘,它只管往 Kafka 发送,并不关心消息是否正确到达。在大多数情况下,这种发送方式没有问题; 不过在某些时候(比如发生不可重试异常时)会造成消息的丢失。这种发送方式的性能最高,可靠性最差。Future<RecordMetadata> send &#x3D; producer.send(rcd);3. 同步发送(sync ) try {	producer.send(rcd).get(); } catch (Exception e) { 	e.printStackTrace(); }(新版中,producer 在底层只有异步)4. 异步发送(async )回调函数会在 producer 收到 ack 时调用,为异步调用,该方法有两个参数,分别是 RecordMetadata 和Exception,如果 Exception 为 null,说明消息发送成功,如果 Exception 不为 null,说明消息发送失败。注意:消息发送失败会自动重试,不需要我们在回调函数中手动重试。<img src="file:///C:\Users\雷神\AppData\Local\Temp\ksohtml9240\wps24.jpg" alt="img"> 四、消费者API1. 一个正常的消费逻辑需要具备以下几个步骤: (1)配置消费者客户端参数(2)创建相应的消费者实例; (3)订阅主题; (4)拉取消息并消费; (5)提交消费位移 offset;(6)关闭消费者实例（代码图片）<img src="file:///C:\Users\雷神\AppData\Local\Temp\ksohtml9240\wps25.jpg" alt="img"><img src="file:///C:\Users\雷神\AppData\Local\Temp\ksohtml9240\wps26.jpg" alt="img"> <img src="file:///C:\Users\雷神\AppData\Local\Temp\ksohtml9240\wps27.jpg" alt="img"> 五、Topic管理API一般情况下,我们都习惯使用 kafka-topic.sh 本来管理主题,如果希望将管理类的功能集成到公司内部的系统中,打造集管理、监控、运维、告警为一体的生态平台,那么就需要以程序调用 API 方式去实现。这种调用 API 方式实现管理主要利用 KafkaAdminClient 工具类KafkaAdminClient 不仅可以用来管理 broker、配置和 ACL (Access Control List),还可用来管理主题)它提供了以下方法:1.列出主题ListTopicsResult listTopicsResult &#x3D; adminClient.listTopics(); Set<String> topics &#x3D; listTopicsResult.names().get(); System.out.println(topics);2.查看主题信息DescribeTopicsResultdescribeTopicsResult&#x3D; &#x3D; adminClient.describeTopics(Arrays.asList(“tpc_3”, “tpc_4”)); Map&lt;String, TopicDescription&gt; res &#x3D; describeTopicsResult.all().get();Set<String> ksets &#x3D; res.keySet(); for (String k : ksets) { 	System.out.println(res.get(k)); }3.创建主题代码示例:&#x2F;&#x2F; 参数配置Properties props &#x3D; new Properties(); props.put(AdminClientConfig.BOOTSTRAP_SERVERS_CONFIG,”node1:9092,node2:9092,node3:9092”); props.put(AdminClientConfig.REQUEST_TIMEOUT_MS_CONFIG,3000); &#x2F;&#x2F; 创建 admin client 对象AdminClient adminClient &#x3D; KafkaAdminClient.create(props); &#x2F;&#x2F; 由服务端 controller 自行分配分区及副本所在 broker NewTopic tpc_3 &#x3D; new NewTopic(“tpc_3”, 2, (short) 1); &#x2F;&#x2F; 手动指定分区及副本的 broker 分配HashMap&lt;Integer, List<Integer>&gt; replicaAssignments &#x3D; new HashMap&lt;&gt;(); &#x2F;&#x2F;分区0分配到broker0,broker1 replicaAssignments.put(0,Arrays.asList(0,1)); &#x2F;&#x2F; 分区 1,分配到 broker0,broker2 replicaAssignments.put(0,Arrays.asList(0,1));NewTopic tpc_4 &#x3D; new NewTopic(“tpc_4”, replicaAssignments); CreateTopicsResultresult&#x3D; &#x3D; adminClient.createTopics(Arrays.asList(tpc_3,tpc_4)); &#x2F;&#x2F; 从 future 中等待服务端返回try { 	result.all().get(); } catch (Exception e) { e.printStackTrace(); } adminClient.close();4.删除主题代码示例: DeleteTopicsResultdeleteTopicsResult&#x3D; &#x3D; adminClient.deleteTopics(Arrays.asList(“tpc_1”, “tpc_1”)); Map&lt;String, KafkaFuture<Void>&gt; values &#x3D; deleteTopicsResult.values(); System.out.println(values);5.除了进行 topic 管理之外,KafkaAdminClient 也可以进行诸如动态参数管理,分区管理等各类管理操作;<img src="file:///C:\Users\雷神\AppData\Local\Temp\ksohtml9240\wps28.jpg" alt="img"></td>
<td></td>
<td></td>
</tr>
<tr>
<td>问题总结</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>成绩</td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody></table>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2022/06/11/Kafka%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/" data-id="cl4ao192z0002akw20q2797fr" data-title="" class="article-share-link">Share</a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-spark搭建" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2022/05/23/spark%E6%90%AD%E5%BB%BA/" class="article-date">
  <time class="dt-published" datetime="2022-05-23T12:42:04.714Z" itemprop="datePublished">2022-05-23</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <hr>
<h2 id="title-我的第一篇博客"><a href="#title-我的第一篇博客" class="headerlink" title="title: 我的第一篇博客"></a>title: 我的第一篇博客</h2><p>Kafka</p>
<p>分组问题报告</p>
<p>班级：      大数据1901   </p>
<p>学号：      194800132 194800117 194800118  </p>
<p>​            194800123                    </p>
<p>姓名：      陈利明 金文祺 周亚东 马永航             </p>
<p>指导教师：      于智龙       </p>
<table>
<thead>
<tr>
<th>分组问题</th>
<th>1</th>
<th>题目</th>
<th>Kafka全流程配置与应用</th>
</tr>
</thead>
<tbody><tr>
<td>问题目的</td>
<td>熟练操作Kafka相关配置、并对相应配置流程、应用逻辑进行梳理。</td>
<td></td>
<td></td>
</tr>
<tr>
<td>问题内容</td>
<td>请各组同学，按以下要求完成一份《Kafka全流程配置与应用》文档。 要求：文档使用word进行编辑（word模板如下）。配置文档需从Kafka环境配置开始，可借鉴课程中的文档进行编写。编写顺序为：1. Kafka环境配置 -&gt;2. Kafka命令行操作 -&gt;3. 生产者API -&gt;4. 消费者API -&gt;5. Topic管理API</td>
<td></td>
<td></td>
</tr>
<tr>
<td>问题进度</td>
<td>本次共有  5  个练习（具体到每个小练习），完成  5   个</td>
<td></td>
<td></td>
</tr>
<tr>
<td>问题设计</td>
<td>任务分配Kafka环境配置陈利明Kafka命令行操作陈利明生产者API周亚东、陈利明消费者API马永航、陈利明Topic管理API金文祺、陈利明 一、Kafka环境配置1.上传kafka_2.11-2.0.0.tgz到export&#x2F;server，使用tar -zxvf kafka_2.11-2.0.0.tgz命令解压<img src="file:///C:\Users\雷神\AppData\Local\Temp\ksohtml8104\wps15.jpg" alt="img"> <img src="file:///C:\Users\雷神\AppData\Local\Temp\ksohtml8104\wps16.jpg" alt="img"> 2. 对解压后的Kafka创建软连接方便访问ln -s &#x2F;export&#x2F;server&#x2F;kafka_2.11-2.0.0 &#x2F;export&#x2F;server&#x2F;kafka<img src="file:///C:\Users\雷神\AppData\Local\Temp\ksohtml8104\wps17.jpg" alt="img">  3. 修改 &#x2F;export&#x2F;server&#x2F;kafka&#x2F;config路径下的配置文件 编辑vi server.properties 修改以下内容broker.id&#x3D;0.从0开始,每台不能重复.Listeners &#x3D; plaintext:&#x2F;&#x2F;:9092通信用的是明文传递改成：Listeners &#x3D; plaintext:&#x2F;&#x2F;node1:9092设置数据存储的目录路径log.dirs&#x3D;&#x2F;export&#x2F;server&#x2F;data&#x2F;kafka-logs默认分区数num.partitions &#x3D; 1指定 zk 集群地址zookeeper.connect&#x3D;node1:2181,node2:2181,node3:2181<img src="file:///C:\Users\雷神\AppData\Local\Temp\ksohtml8104\wps18.jpg" alt="img"> 4. 配置环境变量 &#x2F;etc&#x2F;profile<img src="file:///C:\Users\雷神\AppData\Local\Temp\ksohtml8104\wps19.jpg" alt="img"> source &#x2F;etc&#x2F;profile重新加载环境变量5. cd &#x2F;export&#x2F;server分别把配置文件和环境变量的配置文件分发到node2;node3结点上具体命令如下（内容过长不便截图）scp -r &#x2F;export&#x2F;server&#x2F;kafka&#x2F; node2:$PWDscp -r &#x2F;export&#x2F;server&#x2F;kafka&#x2F; node3:$PWDscp -r &#x2F;etc&#x2F;profile&#x2F; node2:etc&#x2F;profilescp -r &#x2F;etc&#x2F;profile&#x2F; node2:etc&#x2F;profile6. 修改node2和node3的配置文件&#x2F;export&#x2F;server&#x2F;kafka&#x2F;config&#x2F;server.propertiesbroker.id&#x3D;1listeners &#x3D; plaintext:&#x2F;&#x2F;node2:9092<img src="file:///C:\Users\雷神\AppData\Local\Temp\ksohtml8104\wps20.jpg" alt="img"> broker.id&#x3D;2listeners &#x3D; plaintext:&#x2F;&#x2F;node3:9092<img src="file:///C:\Users\雷神\AppData\Local\Temp\ksohtml8104\wps21.jpg" alt="img">  7. 启动Kafka使用脚本一键启动<img src="file:///C:\Users\雷神\AppData\Local\Temp\ksohtml8104\wps22.jpg" alt="img">  二、Kafka命令行操作1. Kafka 中提供了许多命令行工具(位于$KAFKA HOME&#x2F;bin 目录下)用于管理集群的变更。<img src="file:///C:\Users\雷神\AppData\Local\Temp\ksohtml8104\wps23.jpg" alt="img"> 2. 查看当前可用topic.&#x2F;kafka-topics.sh –zookeeper node1:2181,node2:2181,node3:2181 –create –replication-factor 2 –partitions 2 –topic test<img src="file:///C:\Users\雷神\AppData\Local\Temp\ksohtml8104\wps24.jpg" alt="img">  3. 创建topicbin&#x2F;kafka-topics.sh –create –topic tpc_2 –partitions 2 –replication-factor 2 –zookeeper node1:2181该方式下,命令会自动判断所要创建的 topic 的分区数及副本数<img src="file:///C:\Users\雷神\AppData\Local\Temp\ksohtml8104\wps25.jpg" alt="img"> <img src="file:///C:\Users\雷神\AppData\Local\Temp\ksohtml8104\wps26.jpg" alt="img"> 4. 删除topicbin&#x2F;kafka-topics.sh  –delete –topic tpc_3 –zookeeper node1：2181删除 topic,需要一个参数处于启用状态: delete.topic.enable &#x3D; true,否则删不掉<img src="file:///C:\Users\雷神\AppData\Local\Temp\ksohtml8104\wps27.jpg" alt="img"> <img src="file:///C:\Users\雷神\AppData\Local\Temp\ksohtml8104\wps28.jpg" alt="img"> 5.  查看topickafka-topics.sh  –delete –topic tpc_1 –zookeeper node1:2181<img src="file:///C:\Users\雷神\AppData\Local\Temp\ksohtml8104\wps29.jpg" alt="img"> 6. 增加分区数bin&#x2F;kafka-topics.sh –alter –topic tpc_1 –partitions 3 –zookeeper node1:2181需要注意：Kafka 只支持增加分区,不支持减少分区原因是:减少分区,代价太大(数据的转移,日志段拼接合并) 如果真的需要实现此功能,则完全可以重新创建一个分区数较小的主题,然后将现有主题中的消息按照既定的逻辑复制过去; 7. 动态配置topic参数添加、修改配置参数bin&#x2F;kafka-configs.sh –zookeeper node1:2181 –entity-type topics –entity-name tpc_1 –alter –add-config compression.type&#x3D;gzip<img src="file:///C:\Users\雷神\AppData\Local\Temp\ksohtml8104\wps30.jpg" alt="img"> 删除配置参数bin&#x2F;kafka-configs.sh –zookeeper node1:2181 –entity-type topics –entity-name tpc_1 –alter –delete-config compression.type<img src="file:///C:\Users\雷神\AppData\Local\Temp\ksohtml8104\wps31.jpg" alt="img"> 8. Kafka命令行生产者与消费者操作(1)生产者:kafka-console-producerbin&#x2F;kafka-console-producer.sh –broker-list node1:9092, node2:9092, node3:9092 –topic tpc_1<img src="file:///C:\Users\雷神\AppData\Local\Temp\ksohtml8104\wps32.jpg" alt="img">  (2)消费者:kafka-console-consumer1消费消息bin&#x2F;kafka-console-consumer.sh –bootstrap-server node1:9092, node2:9092, node1:9092 –topic tpc_1 –from-beginning(从头开始)<img src="file:///C:\Users\雷神\AppData\Local\Temp\ksohtml8104\wps33.jpg" alt="img"> 2指定要消费的分区,和要消费的起始 offset bin&#x2F;kafka-console-consumer.sh–bootstrap-servernode1:9092,node2:9092,node3:9092 –topic tcp_1 –offset 2 –partition 0 9. Kafka命令行配置管理bin&#x2F;kafka-configs.sh zookeeper node1: 2181 –describe –entity-type topics –entity-name tpc_1 <img src="file:///C:\Users\雷神\AppData\Local\Temp\ksohtml8104\wps34.jpg" alt="img"> 比如查看 broker 的动态配置可以按如下方式执行:bin&#x2F;kafka-configs.sh zookeeper node1: 2181 –describe –entity-type brokers –entity-name 0 –zookeeper node1:2181<img src="file:///C:\Users\雷神\AppData\Local\Temp\ksohtml8104\wps35.jpg" alt="img"> 三、生产者APIProduce流程图<img src="file:///C:\Users\雷神\AppData\Local\Temp\ksohtml8104\wps36.png" alt="img"> 1.一个正常的生产逻辑需要具备以下几个步骤(1)配置生产者客户端参数及创建相应的生产者实例(2)构建待发送的消息(3)发送消息(4)关闭生产者实例创建一个maven工程并导入相关依赖<img src="file:///C:\Users\雷神\AppData\Local\Temp\ksohtml8104\wps37.jpg" alt="img"> 采用默认分区方式将消息散列的发送到各个分区当中（代码图片） Demo详细解释import org.apache.kafka.clients.producer.KafkaProducer; import org.apache.kafka.clients.producer.Producer; import org.apache.kafka.clients.producer.ProducerRecord; import java.util.Properties; public class MyProducer { public static void main(String[ ] args) throws InterruptedException { Properties props &#x3D; new Properties(); &#x2F;&#x2F;设置 kafka 集群的地址\props.put(“bootstrap.servers”, “node1:9092,node2:9092,node3:9092”);&#x2F;&#x2F;ack 模式,取值有 0,1,-1(all) , all 是最慢但最安全的，0不等响应就继续发（可靠性低），1leader会写到本地日志后，然后给响应，producer拿到响应才继续发（follwer还没同步）props.put(“acks”, “all”); props.put(“retries”, 3); &#x2F;&#x2F;失败重试次数-&gt;失败会自动重试（可恢复&#x2F;不可恢复）–&gt;(有可能会造成数据的乱序)props.put(“batch.size”, 10); &#x2F;&#x2F;数据发送的批次大小à提高效率&#x2F;吞吐量à太大会数据延迟props.put(“linger.ms”, 10000); &#x2F;&#x2F;消息在缓冲区保留的时间,超过设置的值就会被提交到服务端props.put(“max.request.size”,10); &#x2F;&#x2F;数据发送请求的最大缓存数props.put(“buffer.memory”, 10240); &#x2F;&#x2F;整个 Producer 用到总内存的大小,如果缓冲区满了会提交数据到服务端&#x2F;&#x2F;buffer.memory 要大于 batch.size,否则会报申请内存不足的错误à降低阻塞的可能性在创建真正的生产者实例前需要配置相应的参数,比如需要连接的 Kafka 集群地址。在 Kafka 生产者客户端 KatkaProducer 中有 3 个参数是必填的。bootstrap.servers key.serializer value.serializer为了防止参数名字符串书写错误,可以使用如下方式进行设置: props.setProperty(ProducerConfig.INTERCEPTOR_CLASSES_CONFIG,ProducerInterceptorPrefix.class.getName());props.setProperty(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG,”node1:9092,node2:9092”); props.setProperty(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG,StringSerializer.class.getName()); （运行结果图片2）2. 生产者api参数发送方式（发后即忘）发后即忘,它只管往 Kafka 发送,并不关心消息是否正确到达。在大多数情况下,这种发送方式没有问题; 不过在某些时候(比如发生不可重试异常时)会造成消息的丢失。这种发送方式的性能最高,可靠性最差。Future<RecordMetadata> send &#x3D; producer.send(rcd);3. 同步发送(sync ) try {	producer.send(rcd).get(); } catch (Exception e) { 	e.printStackTrace(); }(新版中,producer 在底层只有异步)4. 异步发送(async )回调函数会在 producer 收到 ack 时调用,为异步调用,该方法有两个参数,分别是 RecordMetadata 和Exception,如果 Exception 为 null,说明消息发送成功,如果 Exception 不为 null,说明消息发送失败。注意:消息发送失败会自动重试,不需要我们在回调函数中手动重试。<img src="file:///C:\Users\雷神\AppData\Local\Temp\ksohtml8104\wps38.jpg" alt="img"> 四、消费者API1. 一个正常的消费逻辑需要具备以下几个步骤: (1)配置消费者客户端参数(2)创建相应的消费者实例; (3)订阅主题; (4)拉取消息并消费; (5)提交消费位移 offset;(6)关闭消费者实例（代码图片）<img src="file:///C:\Users\雷神\AppData\Local\Temp\ksohtml8104\wps39.jpg" alt="img"><img src="file:///C:\Users\雷神\AppData\Local\Temp\ksohtml8104\wps40.jpg" alt="img"> <img src="file:///C:\Users\雷神\AppData\Local\Temp\ksohtml8104\wps41.jpg" alt="img"> 五、Topic管理API一般情况下,我们都习惯使用 kafka-topic.sh 本来管理主题,如果希望将管理类的功能集成到公司内部的系统中,打造集管理、监控、运维、告警为一体的生态平台,那么就需要以程序调用 API 方式去实现。这种调用 API 方式实现管理主要利用 KafkaAdminClient 工具类KafkaAdminClient 不仅可以用来管理 broker、配置和 ACL (Access Control List),还可用来管理主题)它提供了以下方法:1.列出主题ListTopicsResult listTopicsResult &#x3D; adminClient.listTopics(); Set<String> topics &#x3D; listTopicsResult.names().get(); System.out.println(topics);2.查看主题信息DescribeTopicsResultdescribeTopicsResult&#x3D; &#x3D; adminClient.describeTopics(Arrays.asList(“tpc_3”, “tpc_4”)); Map&lt;String, TopicDescription&gt; res &#x3D; describeTopicsResult.all().get();Set<String> ksets &#x3D; res.keySet(); for (String k : ksets) { 	System.out.println(res.get(k)); }3.创建主题代码示例:&#x2F;&#x2F; 参数配置Properties props &#x3D; new Properties(); props.put(AdminClientConfig.BOOTSTRAP_SERVERS_CONFIG,”node1:9092,node2:9092,node3:9092”); props.put(AdminClientConfig.REQUEST_TIMEOUT_MS_CONFIG,3000); &#x2F;&#x2F; 创建 admin client 对象AdminClient adminClient &#x3D; KafkaAdminClient.create(props); &#x2F;&#x2F; 由服务端 controller 自行分配分区及副本所在 broker NewTopic tpc_3 &#x3D; new NewTopic(“tpc_3”, 2, (short) 1); &#x2F;&#x2F; 手动指定分区及副本的 broker 分配HashMap&lt;Integer, List<Integer>&gt; replicaAssignments &#x3D; new HashMap&lt;&gt;(); &#x2F;&#x2F;分区0分配到broker0,broker1 replicaAssignments.put(0,Arrays.asList(0,1)); &#x2F;&#x2F; 分区 1,分配到 broker0,broker2 replicaAssignments.put(0,Arrays.asList(0,1));NewTopic tpc_4 &#x3D; new NewTopic(“tpc_4”, replicaAssignments); CreateTopicsResultresult&#x3D; &#x3D; adminClient.createTopics(Arrays.asList(tpc_3,tpc_4)); &#x2F;&#x2F; 从 future 中等待服务端返回try { 	result.all().get(); } catch (Exception e) { e.printStackTrace(); } adminClient.close();4.删除主题代码示例: DeleteTopicsResultdeleteTopicsResult&#x3D; &#x3D; adminClient.deleteTopics(Arrays.asList(“tpc_1”, “tpc_1”)); Map&lt;String, KafkaFuture<Void>&gt; values &#x3D; deleteTopicsResult.values(); System.out.println(values);5.除了进行 topic 管理之外,KafkaAdminClient 也可以进行诸如动态参数管理,分区管理等各类管理操作;<img src="file:///C:\Users\雷神\AppData\Local\Temp\ksohtml8104\wps42.jpg" alt="img"></td>
<td></td>
<td></td>
</tr>
<tr>
<td>问题总结</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>成绩</td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody></table>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2022/05/23/spark%E6%90%AD%E5%BB%BA/" data-id="cl4ao19300003akw2dminc0ob" data-title="" class="article-share-link">Share</a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-hello-world" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2022/05/17/hello-world/" class="article-date">
  <time class="dt-published" datetime="2022-05-17T11:29:27.429Z" itemprop="datePublished">2022-05-17</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2022/05/17/hello-world/">Hello World</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>Welcome to <a target="_blank" rel="noopener" href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a target="_blank" rel="noopener" href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a target="_blank" rel="noopener" href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a target="_blank" rel="noopener" href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure>

<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/writing.html">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>

<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/server.html">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>

<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/generating.html">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>

<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2022/05/17/hello-world/" data-id="cl4ao192x0001akw2aw1ebyzc" data-title="Hello World" class="article-share-link">Share</a>
      
      
      
    </footer>
  </div>
  
</article>



  

</section>
        <aside id="sidebar">
  
    

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2022/06/11/Kafka%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA-1/">Kafka环境搭建</a>
          </li>
        
          <li>
            <a href="/2022/06/11/Kafka%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/">(no title)</a>
          </li>
        
          <li>
            <a href="/2022/05/23/spark%E6%90%AD%E5%BB%BA/">(no title)</a>
          </li>
        
          <li>
            <a href="/2022/05/17/hello-world/">Hello World</a>
          </li>
        
      </ul>
    </div>
  </div>

  
    

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/06/">June 2022</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/05/">May 2022</a></li></ul>
    </div>
  </div>

  
</aside>
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2022 By Autoload<br>
      Driven - <a href="https://hexo.io/" target="_blank">Hexo</a>|Theme - <a href="https://github.com/autoload/hexo-theme-auto" target="_blank">Auto</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
    <a href="/categories" class="mobile-nav-link">Categories</a>
  
    <a href="/tags" class="mobile-nav-link">Tags</a>
  
    <a href="/about" class="mobile-nav-link">About</a>
  
</nav>
    


<script src="/js/jquery-3.4.1.min.js"></script>


<script src="/js/script.js"></script>




  </div>
</body>
</html>